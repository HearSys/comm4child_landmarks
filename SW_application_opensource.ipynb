{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1249e9cc-72c8-41a6-8f2b-f51bf3541cda",
   "metadata": {},
   "source": [
    "# Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1364f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "numpy.float = float\n",
    "numpy.int = numpy.int_\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "from scipy import ndimage as ndi\n",
    "from tqdm.notebook import tqdm\n",
    "import neuclid as nc\n",
    "from volume import Volume\n",
    "from scipy import ndimage\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import logging\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5858423",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"load data generator for trainig and testing data\"\"\"\n",
    "import Data_generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a98af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina' # to make the plot more resolution\n",
    "plt.style.use(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4436fe-aff5-4b0b-bfbe-a85bd2eeced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable GPU\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "tf.config.set_soft_device_placement(True)\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "sess = tf.compat.v1.Session()\n",
    "print(device_lib.list_local_devices())\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "# test if GPU is enabled on local device: macOS m1 mps, Windows GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
    "print(\"Device Name:\", tf.test.gpu_device_name())\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices(\"GPU\")))\n",
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e74f99-d3c8-4a07-aa32-85e3831d63ec",
   "metadata": {},
   "source": [
    "# Load volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12e2c551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61c4f843f7c4b2d994bddc247b277f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0002_toload\n",
      "testvolume/0002_toload/0002.nrrd\n",
      "\tleftpath.parent.name[:4] is 0002\n",
      "path  is testvolume/0002_toload/0002.nrrd\n",
      "\tgood segmentation: GT\n",
      "cochlea_fcsv_files is [PosixPath('testvolume/0002_toload/left/GT/Cochlea.fcsv')]\n",
      "\t -> ['Cochlea.fcsv']\n",
      "\n",
      "\trightpath.parent.name[:4] is 0002\n",
      "path  is testvolume/0002_toload/0002.nrrd\n",
      "\tgood segmentation: GT\n",
      "cochlea_fcsv_files is [PosixPath('testvolume/0002_toload/right/GT/Cochlea_r.fcsv')]\n",
      "\t -> ['Cochlea_r.fcsv']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nrrd_volumes, cochlea_l, cochlea_r = Data_generator.volume_ct_load(Data_generator.base_dir) # here to take the loaded volumes ground truch data, with the default author provided volume data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa366c-5fc0-4c41-93da-a2c2da34fcd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Start the training on the fly (Optional: activate it when a local model needs to be trained) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"training function to train from scratch the SMICNet locally, defaut set to False \"\"\"\n",
    "train_activate=False  # set to True to train the model, this will take a long duration to train \n",
    "if train_activate:\n",
    "    import Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3122fa-74d9-4df6-9bf1-4bc09d6b5faf",
   "metadata": {},
   "source": [
    "# Model application/inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56c8f99",
   "metadata": {},
   "source": [
    "### load different model trained with different quantity volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c8d674",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SW_4classes_trainedwith1volume = keras.models.load_model(\n",
    "    \"TrainedNetworkWeights/model_epoch_100_1volume_authorprovided.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SW_4classes_trainedwith3volumes = keras.models.load_model(\n",
    "    \"TrainedNetworkWeights/model_epoch_100_3volumes_authorprovided.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8b7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SW_4classes_trainedwith5volumes = keras.models.load_model(\n",
    "    \"TrainedNetworkWeights/model_epoch_100_5volumes_authorprovided.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032271cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SW_4classes_trainedwith7volumes = keras.models.load_model(\n",
    "    \"TrainedNetworkWeights/model_epoch_100_7volumes_authorprovided.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SW_4classes_trainedwith9volumes = keras.models.load_model(\n",
    "    \"TrainedNetworkWeights/model_epoch_100_9volumes_authorprovided.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec4997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_SW_4classes_trainedwith11volumes = keras.models.load_model(\n",
    "    \"TrainedNetworkWeights/model_epoch_100_11volumes_authorprovided.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b94b51-6d1e-430b-b88f-081566f4d891",
   "metadata": {},
   "source": [
    "## classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdccfc9-739a-4f63-998e-240bb6211a5c",
   "metadata": {},
   "source": [
    "### Evaluation of images from generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrrd_volumes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b27e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen11 = Data_generator.Data_augmentation_Generator(\n",
    "    batch_size=32,\n",
    "    selected_volumes=['0002'], # train_ids, choose based on the nrrd_volumes.keys()\n",
    "    steps1=4,\n",
    "    steps2=4,\n",
    "    steps3=4,#to adjust how many 2d data to generate to test\n",
    "    alpha_range=10,\n",
    "    beta_range=10,\n",
    "    gamma_range=330,\n",
    "    window_size=81,\n",
    "    gradual_rotated_padding_size=3,\n",
    "    rotated_augmentation_size=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f74740",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(test_data_gen11)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e7d2b-ed6f-4778-b917-22da1e512ea5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "apex_SW12 = model_SW_4classes_trainedwith5volumes.predict(x)\n",
    "apex_SW12  # epoch 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa53243-fec1-4195-bf62-bcb917c39d49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y  # to compare the ground truth label of this batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f67cf-b5fa-4223-97a1-7e09f5b71036",
   "metadata": {},
   "source": [
    "### quick look which class the loaded 2D sub image belongs to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea5047-4651-47ba-9cec-4d08010c413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_class(label):\n",
    "    if label == 0:\n",
    "        return (\"This is class cochlea apex! \")\n",
    "    elif label == 1:\n",
    "        return (\"This is class cochlea basal!\")\n",
    "    elif label == 2:\n",
    "        return (\"This is class cochlea round window!\")\n",
    "    elif label == 3:\n",
    "        return (\"This is class background!\")\n",
    "    else: \n",
    "        print(\"Invalid class,there is an error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3af409-70a2-49b9-bd70-73d31869b8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_and_class(i):\n",
    "    input_2dimg=tf.expand_dims(x[i], axis=0)\n",
    "    predicted_2dlabel=model_SW_4classes_trainedwith5volumes.predict(input_2dimg)\n",
    "\n",
    "    plt.imshow(x[i].squeeze(), cmap='gray')\n",
    "    plt.show()\n",
    "    print(\"Ground truth class:\", which_class(np.argmax(y[i])))\n",
    "    print(\"Predicted class:\", which_class(np.argmax(predicted_2dlabel)))\n",
    "    if np.argmax(y[i])== np.argmax(predicted_2dlabel):\n",
    "        print(\"The SMICNet model is right on this given image's class!!\")\n",
    "    else:\n",
    "        print(\"The SMICNet model made a mistake on this given image's class!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe29bf-0f51-4585-b708-edad7db15e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_and_class(1) # here can change the int value to switch the image to test, here shows the class of the center of the image loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59219675-422a-492c-bd77-a42b4eb3357e",
   "metadata": {},
   "source": [
    "### Optional: CNN layers visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd819fc-464f-4327-9439-499c614eac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "layyer_visualisation=False # optional: change to True to see each layer's features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6533a21c-0ba0-42b8-abe4-4be5bf6feedd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if layyer_visualisation:\n",
    "    x_input = tf.expand_dims(x[0], axis=0)\n",
    "    layer_outputs = []\n",
    "    for layer in model_SW_4classes_trainedwith5volumes.layers:\n",
    "        layer_model = tf.keras.Model(\n",
    "            inputs=model_SW_4classes_trainedwith5volumes.input, outputs=layer.output\n",
    "        )\n",
    "        output = layer_model.predict(x_input)\n",
    "        layer_outputs.append(output)\n",
    "\n",
    "\n",
    "    n_layers = len(layer_outputs)\n",
    "    n_cols = 8\n",
    "    n_rows = int(np.ceil(n_layers / n_cols))\n",
    "\n",
    "    fig, axs = plt.subplots(n_rows, n_cols, figsize=(2 * n_cols, 2 * n_rows))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, ax in enumerate(axs):\n",
    "        if i < n_layers:\n",
    "            output = layer_outputs[i]\n",
    "\n",
    "            if output.ndim == 4:\n",
    "                ax.imshow(output[0, :, :, 0])\n",
    "            elif output.ndim == 2:\n",
    "                ax.imshow(output, cmap=\"viridis\")\n",
    "            else:\n",
    "                ax.text(\n",
    "                    0.5,\n",
    "                    0.5,\n",
    "                    f\"Cannot display\\noutput of shape\\n{output.shape}\",\n",
    "                    horizontalalignment=\"center\",\n",
    "                    verticalalignment=\"center\",\n",
    "                    fontsize=12,\n",
    "                )\n",
    "\n",
    "            ax.axis(\"off\")\n",
    "            ax.set_title(f\"Layer {i + 1}\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "    plt.savefig(\n",
    "        f\"layervisualisation_batch1images.png\",\n",
    "        format=\"png\",\n",
    "        dpi=400,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add2e32-4fde-4058-bbee-bb6fc14445ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classification_score_formatready(\n",
    "    set_toselect,\n",
    "    quantity_nrrdstochoose,\n",
    "    steps1,\n",
    "    steps2,\n",
    "    steps3,\n",
    "    model,\n",
    "    model_indice,\n",
    "    batch_size=32,\n",
    "):\n",
    "    df_scores_list = list()\n",
    "    f1_scores_per_class_list = list()\n",
    "    f1_macro_list = list()\n",
    "    f1_imbalanced_differentclass_class_list = list()\n",
    "    f1_weighted_list = list()\n",
    "    prob_scores_list2_array_list = list()\n",
    "    y_pred_labelrray_list = list()\n",
    "    y_true_list2_array_list = list()\n",
    "    data_list = list()\n",
    "    for i in range(len(set_toselect) - quantity_nrrdstochoose):\n",
    "        selected_volumes = [set_toselect[i]]\n",
    "        # print(selected_volumes)\n",
    "        y_true_list = list()\n",
    "        prob_scores_list = list()\n",
    "        y_true_list2 = list()\n",
    "        y_pred_label = list()\n",
    "        prob_scores_list2 = list()\n",
    "\n",
    "        batch_size = batch_size\n",
    "\n",
    "        test_data_gen11 = Data_generator.Data_augmentation_Generator(\n",
    "            batch_size=batch_size,\n",
    "            selected_volumes=selected_volumes,  # train_ids\n",
    "            steps1=steps1,\n",
    "            steps2=steps2,\n",
    "            steps3=steps3,\n",
    "            alpha_range=10,\n",
    "            beta_range=10,\n",
    "            gamma_range=330,\n",
    "            window_size=81,\n",
    "            gradual_rotated_padding_size=17,\n",
    "            rotated_augmentation_size=10,\n",
    "        )\n",
    "        for row in range(len(test_data_gen11)):\n",
    "            img_, y_true = next(iter(test_data_gen11))\n",
    "            prob_scores = model.predict(img_)\n",
    "            y_true_list.append(y_true)\n",
    "            prob_scores_list.append(prob_scores)\n",
    "\n",
    "        for i in y_true_list:\n",
    "            for j in range(batch_size):\n",
    "                y_true_list2.append(np.argmax(i[j]))\n",
    "\n",
    "        y_true_list2_array = np.array(y_true_list2)\n",
    "        for i in prob_scores_list:\n",
    "            for j in range(batch_size):\n",
    "                prob_scores_list2.append(i[j])\n",
    "                y_pred_label.append(i[j])\n",
    "\n",
    "        prob_scores_list2_array = np.array(prob_scores_list2)\n",
    "        y_true_list2_array_list.append(y_true_list2_array)\n",
    "\n",
    "        y_pred_labelrray = np.array(y_pred_label)\n",
    "        y_pred_label = np.argmax(y_pred_labelrray, axis=1)\n",
    "\n",
    "        prob_scores_list2_array_list.append(prob_scores_list2_array)\n",
    "        y_pred_labelrray_list.append(y_pred_labelrray)\n",
    "\n",
    "        f1_scores_per_class = f1_score(y_true_list2_array, y_pred_label, average=None)\n",
    "        precision_per_class = precision_score(\n",
    "            y_true_list2_array, y_pred_label, average=None\n",
    "        )\n",
    "        recall_per_class = recall_score(y_true_list2_array, y_pred_label, average=None)\n",
    "\n",
    "        f1_macro = f1_score(y_true_list2_array, y_pred_label, average=\"macro\")\n",
    "        f1_imbalanced_differentclass = f1_score(\n",
    "            y_true_list2_array, y_pred_label, average=\"micro\"\n",
    "        )\n",
    "        f1_weighted = f1_score(y_true_list2_array, y_pred_label, average=\"weighted\")\n",
    "\n",
    "        f1_scores_per_class_list.append([selected_volumes] + [f1_scores_per_class])\n",
    "        f1_macro_list.append([selected_volumes] + [f1_macro])\n",
    "        f1_imbalanced_differentclass_class_list.append(\n",
    "            [selected_volumes] + [f1_imbalanced_differentclass]\n",
    "        )\n",
    "        f1_weighted_list.append([selected_volumes] + [f1_weighted])\n",
    "\n",
    "        print(f\"f1_scores_per_class is {f1_scores_per_class}\")\n",
    "\n",
    "        for i, f1_per_class in enumerate(f1_scores_per_class):\n",
    "            if i == 0:\n",
    "                class_label = \"Apex\"\n",
    "            elif i == 1:\n",
    "                class_label = \"Basal\"\n",
    "            elif i == 2:\n",
    "                class_label = \"Round window\"\n",
    "            else:\n",
    "                class_label = \"Background\"\n",
    "            data_row = {\n",
    "                \"selected Volume\": selected_volumes,\n",
    "                \"Class\": class_label,\n",
    "                \"Precision per Class\": precision_per_class[i],\n",
    "                \"Recall per Class\": recall_per_class[i],\n",
    "                \"F1 per Class\": f1_per_class,\n",
    "                \"F1 Macro\": f1_macro,\n",
    "                \"F1 Micro\": f1_imbalanced_differentclass,\n",
    "                \"F1 Weighted\": f1_weighted,\n",
    "            }\n",
    "            data_list.append(data_row)\n",
    "        df_scores = pd.DataFrame(data_list)\n",
    "\n",
    "        df_scores.to_csv(\n",
    "            f\"f1_scores_SMICNet_0{model_indice}.csv\",\n",
    "            index=False,\n",
    "        )\n",
    "    return (\n",
    "        df_scores,\n",
    "        y_true_list2_array_list,\n",
    "        f1_scores_per_class_list,\n",
    "        f1_macro_list,\n",
    "        f1_imbalanced_differentclass_class_list,\n",
    "        f1_weighted_list,\n",
    "        prob_scores_list2_array_list,\n",
    "        y_pred_labelrray_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141f2b1-c4fd-49e9-9c74-55bf94a3660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose the volume data index to test \n",
    "set_toselect = [\n",
    "    \"0002\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0334a3-9169-4dbd-838f-f7a15f059d09",
   "metadata": {},
   "source": [
    "### Optional: F1 score generation based on 2D images, with different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e9949-6d00-40b6-96e0-2b88462200bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model trained with 1 volume\n",
    "(\n",
    "    df_scores_list,\n",
    "    y_true_list2_array_list,\n",
    "    f1_scores_per_class_list,\n",
    "    f1_macro_list,\n",
    "    f1_imbalanced_differentclass_class_list,\n",
    "    f1_weighted_list,\n",
    "    prob_scores_list2_array_list,\n",
    "    y_pred_labelrray_list,\n",
    ") = classification_score_formatready(\n",
    "    set_toselect,\n",
    "    quantity_nrrdstochoose=0,\n",
    "    steps1=4,\n",
    "    steps2=4,\n",
    "    steps3=4,  # can change here to control the quantity of data used for the f1 score calculation\n",
    "    model=model_SW_4classes_trainedwith1volume,\n",
    "    model_indice=1,  # indice of trained volume's quantity\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d74a84-cf3e-406b-aa7f-b5e214347c25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model trained with 3 volumes\n",
    "(\n",
    "    df_scores_list,\n",
    "    y_true_list2_array_list,\n",
    "    f1_scores_per_class_list,\n",
    "    f1_macro_list,\n",
    "    f1_imbalanced_differentclass_class_list,\n",
    "    f1_weighted_list,\n",
    "    prob_scores_list2_array_list,\n",
    "    y_pred_labelrray_list,\n",
    ") = classification_score_formatready(\n",
    "    set_toselect,\n",
    "    quantity_nrrdstochoose=0,\n",
    "    steps1=4,\n",
    "    steps2=4,\n",
    "    steps3=4,\n",
    "    model=model_SW_4classes_trainedwith3volumes,\n",
    "    model_indice=3,  # indice of trained volume's quantity\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccc96c-f2a5-47dd-80d3-b907238fa922",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model trained with 5 volumes\n",
    "(\n",
    "    df_scores_list,\n",
    "    y_true_list2_array_list,\n",
    "    f1_scores_per_class_list,\n",
    "    f1_macro_list,\n",
    "    f1_imbalanced_differentclass_class_list,\n",
    "    f1_weighted_list,\n",
    "    prob_scores_list2_array_list,\n",
    "    y_pred_labelrray_list,\n",
    ") = classification_score_formatready(\n",
    "    set_toselect,\n",
    "    quantity_nrrdstochoose=0,\n",
    "    steps1=4,\n",
    "    steps2=4,\n",
    "    steps3=4,\n",
    "    model=model_SW_4classes_trainedwith5volumes,\n",
    "    model_indice=5,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf51246-92fc-439d-b5ff-65a246684712",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model trained with 7 volumes\n",
    "(\n",
    "    df_scores_list,\n",
    "    y_true_list2_array_list,\n",
    "    f1_scores_per_class_list,\n",
    "    f1_macro_list,\n",
    "    f1_imbalanced_differentclass_class_list,\n",
    "    f1_weighted_list,\n",
    "    prob_scores_list2_array_list,\n",
    "    y_pred_labelrray_list,\n",
    ") = classification_score_formatready(\n",
    "    set_toselect,\n",
    "    quantity_nrrdstochoose=0,\n",
    "    steps1=4,\n",
    "    steps2=4,\n",
    "    steps3=4,\n",
    "    model=model_SW_4classes_trainedwith7volumes,\n",
    "    model_indice=7,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6164e-0e3d-429b-8067-46394075c81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model trained with 9 volumes\n",
    "(\n",
    "    df_scores_list,\n",
    "    y_true_list2_array_list,\n",
    "    f1_scores_per_class_list,\n",
    "    f1_macro_list,\n",
    "    f1_imbalanced_differentclass_class_list,\n",
    "    f1_weighted_list,\n",
    "    prob_scores_list2_array_list,\n",
    "    y_pred_labelrray_list,\n",
    ") = classification_score_formatready(\n",
    "    set_toselect,\n",
    "    quantity_nrrdstochoose=0,\n",
    "    steps1=4,\n",
    "    steps2=4,\n",
    "    steps3=4,\n",
    "    model=model_SW_4classes_trainedwith9volumes,\n",
    "    model_indice=9,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af1532-f1d1-4d43-b7c0-14732984a1be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model trained with 11 volumes\n",
    "(\n",
    "    df_scores_list,\n",
    "    y_true_list2_array_list,\n",
    "    f1_scores_per_class_list,\n",
    "    f1_macro_list,\n",
    "    f1_imbalanced_differentclass_class_list,\n",
    "    f1_weighted_list,\n",
    "    prob_scores_list2_array_list,\n",
    "    y_pred_labelrray_list,\n",
    ") = classification_score_formatready(\n",
    "    set_toselect,\n",
    "    quantity_nrrdstochoose=0,\n",
    "    steps1=4,\n",
    "    steps2=4,\n",
    "    steps3=4,\n",
    "    model=model_SW_4classes_trainedwith11volumes,\n",
    "    model_indice=11,\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30df6c1",
   "metadata": {},
   "source": [
    "###  Optional: load author provided F1 score CSV, to compare classification performance among different networks \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbcc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsF1_score=False # optional: change to True to see the F1 score of different model trained with different volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52052cab-cc23-4cbd-b568-6c2e783cca05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if weightsF1_score:\n",
    "    # Load the CSV file on model trained with 1 volume\n",
    "    # df_f1scores = pd.read_csv(\n",
    "    #     \"f1_scores_SMICNet_01_authorprovided.csv\", on_bad_lines=\"skip\"\n",
    "    # )\n",
    "    # # Load the CSV file on model trained with 5 volumes\n",
    "    df_f1scores = pd.read_csv(\n",
    "        \"f1_scores_SMICNet_05_authorprovided.csv\", on_bad_lines=\"skip\"\n",
    "    )\n",
    "    # # Load the CSV file on model trained with 11 volumes\n",
    "    # df_f1scores = pd.read_csv(\n",
    "    #     \"f1_scores_SMICNet_011_authorprovided.csv\", on_bad_lines=\"skip\"\n",
    "    # )\n",
    "else:\n",
    "    df_f1scores = None\n",
    "df_f1scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972643ae-97be-4c65-806b-5a9cfa431781",
   "metadata": {},
   "source": [
    "## Distance: run on the 3D volume "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bfdcc1-e175-4908-a4c9-363f4b564eee",
   "metadata": {},
   "source": [
    "### Sliding Window process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7c694-8c4b-4944-b128-d149459876e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfimage_SW(img):\n",
    "    window = tf.keras.preprocessing.image.array_to_img(img)\n",
    "    window = tf.keras.preprocessing.image.img_to_array(window)\n",
    "    window_input = tf.expand_dims(\n",
    "        window, axis=0\n",
    "    )  \n",
    "    window_input = window_input / 255.0\n",
    "    return window_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91aa04-fecd-43c2-b707-68e5afef254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_range_creating(range_set=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    to choose \n",
    "    \"\"\"\n",
    "    x, y, z = (\n",
    "        np.arange(-range_set, range_set),\n",
    "        np.arange(-range_set, range_set),\n",
    "        np.arange(-range_set, range_set),\n",
    "    )\n",
    "    xx, yy, zz = np.meshgrid(x, y, z)\n",
    "    xy_coords_r_2darray = np.column_stack(\n",
    "        (xx.ravel(), yy.ravel(), zz.ravel())\n",
    "    )  \n",
    "    return xy_coords_r_2darray  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5795045-b092-46e0-b9f2-4c11ee195663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SW_volume_loop_sagittal_multiclass_coarse(\n",
    "    reference,\n",
    "    v,\n",
    "    SW_model,\n",
    "    corse_step=3,\n",
    "    fine_step=1,\n",
    "    search_range=3,\n",
    "    range_set=5,\n",
    "    coarse_thresh=0.92,\n",
    "):\n",
    "    \"\"\"\n",
    "    sliding proceess: \n",
    "    \n",
    "    - do a first coarse sliding:  per 3 pixel, if find interesting pixels (coarse_thresh),\n",
    "    do a search per 1 pixel for a range of 6 pixel (with pixel of interest in the middle)\n",
    "\n",
    "    this function is to slide following the saggital direction, can also change it to coronal or axial direction\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    xy_coords_2darray = sliding_range_creating(range_set=range_set)\n",
    "    out_origin_class1 = np.zeros_like(v.data, dtype=\"float\")\n",
    "    out_origin_class2 = np.zeros_like(v.data, dtype=\"float\")\n",
    "    out_origin_class3 = np.zeros_like(v.data, dtype=\"float\")\n",
    "\n",
    "    coordinates_correspond_list_SW = []\n",
    "    global_range = xy_coords_2darray + reference\n",
    "    selstected_range = global_range[::corse_step]\n",
    "    for index, element in enumerate(selstected_range):\n",
    "        x1, y1, z1 = element\n",
    "        window_see = v.data[\n",
    "            round(x1),\n",
    "            round(y1) - 40 : round(y1) + 41,\n",
    "            round(z1) - 40 : round(z1) + 41,\n",
    "        ]\n",
    "        window_see = tf.expand_dims(window_see, axis=2)\n",
    "\n",
    "        window_input = tfimage_SW(window_see)\n",
    "        SW_proba = SW_model.predict(window_input)\n",
    "        if SW_proba[0][3] > coarse_thresh:\n",
    "            print(\"background!\")\n",
    "        else:\n",
    "            start = max(0, index * corse_step - search_range)\n",
    "            end = min(len(global_range), index * corse_step + search_range + 1)\n",
    "            print(f\"global_range[start:end] is {global_range[start:end]}\")\n",
    "\n",
    "            for x2, y2, z2 in tqdm(global_range[start:end]):\n",
    "                window_see2 = v.data[\n",
    "                    round(x2),\n",
    "                    round(y2) - 40 : round(y2) + 41,\n",
    "                    round(z2) - 40 : round(z2) + 41,\n",
    "                ]\n",
    "                window_see2 = tf.expand_dims(window_see2, axis=2)\n",
    "\n",
    "                window_input2 = tfimage_SW(window_see2)\n",
    "\n",
    "                SW_proba2 = SW_model.predict(window_input2)\n",
    "                class_label = np.argmax(SW_proba2, axis=1)\n",
    "                coordinates_correspond = [x2, y2, z2]\n",
    "                coordinates_correspond_list_SW.append(\n",
    "                    [class_label] + [SW_proba2] + [coordinates_correspond]\n",
    "                )\n",
    "\n",
    "                if class_label == array([0]):\n",
    "                    out_origin_class1[round(x2), round(y2), round(z2)] = np.max(\n",
    "                        SW_proba2\n",
    "                    )\n",
    "                elif class_label == array([1]):\n",
    "                    out_origin_class2[round(x2), round(y2), round(z2)] = np.max(\n",
    "                        SW_proba2\n",
    "                    )\n",
    "                elif class_label == array([2]):\n",
    "                    out_origin_class3[round(x2), round(y2), round(z2)] = np.max(\n",
    "                        SW_proba2\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"background! step2\")\n",
    "\n",
    "    return (\n",
    "        coordinates_correspond_list_SW,\n",
    "        out_origin_class1,\n",
    "        out_origin_class2,\n",
    "        out_origin_class3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d376660-0519-44a9-8f5c-0a2375d63a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_generation(\n",
    "    v,\n",
    "    cochlea_center_l,\n",
    "    cochlea_center_l_apex,\n",
    "    cochlea_center_l_rw,\n",
    "    selected_volumes,\n",
    "    left_ijk,\n",
    "    left_ijk_apex,\n",
    "    left_ijk_rw,\n",
    "    cochlea_center_r,\n",
    "    cochlea_center_r_apex,\n",
    "    cochlea_center_r_rw,\n",
    "    right_ijk,\n",
    "    right_ijk_apex,\n",
    "    right_ijk_rw,\n",
    "    SW_model,\n",
    "    model_indice=0,\n",
    "    range_set=12,\n",
    "):\n",
    "    \"\"\"for each trained weight,generate different probability map for comparasion\"\"\"\n",
    "    out_origin_class1_saggitale_3volumes_left = ([],)\n",
    "    out_origin_class2_saggitale_3volumes_left = ([],)\n",
    "    out_origin_class3_saggitale_3volumes_left = ([],)\n",
    "    initial_pointleft = (left_ijk + left_ijk_apex + left_ijk_rw) / 3\n",
    "    initial_pointright = (right_ijk + right_ijk_apex + right_ijk_rw) / 3 # can do the same for the right side \n",
    "\n",
    "    \"\"\"\"LR sagittal\"\"\"\n",
    "\n",
    "    (\n",
    "        coordinates_correspond_list_SW_saggitale_3volumes_left,\n",
    "        out_origin_class1_saggitale_3volumes_left,\n",
    "        out_origin_class2_saggitale_3volumes_left,\n",
    "        out_origin_class3_saggitale_3volumes_left,\n",
    "    ) = SW_volume_loop_sagittal_multiclass_coarse(\n",
    "        initial_pointleft,\n",
    "        v,\n",
    "        range_set=range_set,\n",
    "        SW_model=SW_model,\n",
    "        coarse_thresh=0.5,\n",
    "    )\n",
    "\n",
    "    np.save(\n",
    "        f\"m{model_indice}_probabilitymap_class1_{selected_volumes}_saggital.npy\",\n",
    "        out_origin_class1_saggitale_3volumes_left,\n",
    "    )  # save\n",
    "    np.save(\n",
    "        f\"m{model_indice}_probabilitymap_class2_{selected_volumes}_saggital.npy\",\n",
    "        out_origin_class2_saggitale_3volumes_left,\n",
    "    )  # save\n",
    "    np.save(\n",
    "        f\"m{model_indice}_probabilitymap_class3_{selected_volumes}_saggital.npy\",\n",
    "        out_origin_class3_saggitale_3volumes_left,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f8b34e-6511-468d-a239-fb02f4df6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmark_locations_SW_centermass(\n",
    "    volume=None, probability_map=None, threshold=0.9\n",
    "):\n",
    "    \"\"\"\n",
    "    get center mass: take the centeroid position\n",
    "\n",
    "    24/05/23\n",
    "    \"\"\"\n",
    "    labels, n_labels = ndimage.label(probability_map > threshold)\n",
    "    prob_map = probability_map.copy()\n",
    "    prob_map[prob_map < threshold] = 0\n",
    "    print(f\"N = {n_labels} landmarks found.\")  \n",
    "    islands_sizes = []\n",
    "    for n in range(\n",
    "        1, n_labels + 1\n",
    "    ):  # bigger center mass will be chosen if there are 2 island(n_lables)\n",
    "\n",
    "        size_of_island = len(labels[labels == n])\n",
    "        islands_sizes.append([size_of_island, n])\n",
    "    print(f\"islands_sizes {islands_sizes}\")\n",
    "    if len(islands_sizes) == 0:\n",
    "        return None\n",
    "    largest_island_size, largest_island_label = list(sorted(islands_sizes))[-1]\n",
    "    prob_map[\n",
    "        labels != largest_island_label\n",
    "    ] = 0  # we set the prob_map to zero everywhere except where the largest island is\n",
    "    center_of_mass = ndi.center_of_mass(prob_map)\n",
    "\n",
    "    print(f\"landmark as center of mass in ijk = {center_of_mass}\")\n",
    "    if volume:\n",
    "        landmark = volume.to_world(\n",
    "            center_of_mass\n",
    "        ) #ras coordinates\n",
    "\n",
    "    else:\n",
    "        landmark = center_of_mass\n",
    "\n",
    "    return landmark, largest_island_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb4c46-9371-47f8-80cc-105436f61050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_mm(v, landmarks_coordinates, ground_truth_coordinates):\n",
    "\n",
    "    # convert ijk distance to distance in mm\n",
    "    predicted_point = nc.Point3(v.to_world(list(landmarks_coordinates)))\n",
    "    return np.linalg.norm(predicted_point - np.array(ground_truth_coordinates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decdf1d-aa4c-4d41-9f14-384c1c281a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    v_loaded,\n",
    "    cochlea_center_l_loaded,\n",
    "    cochlea_center_l_loaded_apex,\n",
    "    cochlea_center_l_loaded_rw,\n",
    "    left_ijk_loaded,\n",
    "    left_ijk_loaded_apex,\n",
    "    left_ijk_loaded_rw,\n",
    "    cochlea_center_r_loaded,\n",
    "    cochlea_center_r_loaded_apex,\n",
    "    cochlea_center_r_loaded_rw,\n",
    "    right_ijk_loaded,\n",
    "    right_ijk_loaded_apex,\n",
    "    right_ijk_loaded_rw,\n",
    ") = Data_generator.load_volume_ijk_GT(\"0002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e17e4f-64a4-4330-94a3-bc70e97c54d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_pointleft_loaded = (\n",
    "    left_ijk_loaded + left_ijk_loaded_apex + left_ijk_loaded_rw\n",
    ") / 3  # center of these 3 landmarks, consider as intial manual input\n",
    "initial_pointleft_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6545c01-f22a-4e27-8e53-19a45383d3f5",
   "metadata": {},
   "source": [
    "### probability map generateration: with ex model trained with 5 volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a19bab-0e4d-4baa-ac94-7a4152468c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# genereate maps with different loaded volume\n",
    "set_toselect = [\"0002\"]\n",
    "quantity_nrrdstochoose=0 # can set the range of how many volumes' probability map to do \n",
    "for i in range(len(set_toselect) - quantity_nrrdstochoose):\n",
    "    selected_volumes = set_toselect[i]\n",
    "    map_generation(\n",
    "        v=v_loaded,\n",
    "        cochlea_center_l=cochlea_center_l_loaded,\n",
    "        cochlea_center_l_apex=cochlea_center_l_loaded_apex,\n",
    "        cochlea_center_l_rw=cochlea_center_l_loaded_rw,\n",
    "        selected_volumes=selected_volumes,\n",
    "        left_ijk=left_ijk_loaded,\n",
    "        left_ijk_apex=left_ijk_loaded_apex,\n",
    "        left_ijk_rw=left_ijk_loaded_rw,\n",
    "        cochlea_center_r=cochlea_center_r_loaded,\n",
    "        cochlea_center_r_apex=cochlea_center_r_loaded_apex,\n",
    "        cochlea_center_r_rw=cochlea_center_r_loaded_rw,\n",
    "        right_ijk=right_ijk_loaded,\n",
    "        right_ijk_apex=right_ijk_loaded_apex,\n",
    "        right_ijk_rw=right_ijk_loaded_rw,\n",
    "        SW_model=model_SW_4classes_trainedwith5volumes,#switch trained model\n",
    "        model_indice=5,#switch which model\n",
    "        range_set=12, # here can adjust the range(voxel) of interest, lower the value, quicker the inference, but may skip the actual fiducial point\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7391026-03bb-4395-af8c-7ccf5d162d52",
   "metadata": {},
   "source": [
    "### load trained probability map for calcuate the landamrk coordinates: for 3 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e165dc-f757-4323-bfb4-4a8954d6b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilitymap_volume_class1 = np.load(\n",
    "    \"m5_probabilitymap_class1_0002_saggital.npy\"\n",
    ")\n",
    "probabilitymap_volume_class2 = np.load(\n",
    "    \"m5_probabilitymap_class2_0002_saggital.npy\"\n",
    ")\n",
    "probabilitymap_volume_class3 = np.load(\n",
    "    \"m5_probabilitymap_class3_0002_saggital.npy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6e7cdd-5657-4999-93b8-485fe6ab468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilitymap_volume_class3.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef290285-7831-4d74-ad28-9fca9c0e8159",
   "metadata": {},
   "source": [
    "### predicted landmark localization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380d700-92dd-49bf-bc50-03e130c46eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apex##\n",
    "landmarks_predicted_class1 = get_landmark_locations_SW_centermass(\n",
    "    probability_map=probabilitymap_volume_class1,\n",
    "    threshold=0.9999,\n",
    ")\n",
    "print(\n",
    "    f\" predicted coordinates is {landmarks_predicted_class1} and GT coordinate is {left_ijk_loaded_apex}\"\n",
    ")\n",
    "\n",
    "\n",
    "##basal##\n",
    "# landmarks_predicted_class2 = get_landmark_locations_SW_centermass(\n",
    "#     probability_map=probabilitymap_volume_class2,\n",
    "#     threshold=0.99,\n",
    "# )\n",
    "\n",
    "# print(f\" predicted coordinates is {landmarks_predicted_class2} and GT coordinate is {left_ijk_loaded}\")\n",
    "\n",
    "##rw####\n",
    "# landmarks_predicted_class3 = get_landmark_locations_SW_centermass(\n",
    "#     probability_map=probabilitymap_volume_class3,\n",
    "#     threshold=0.5,\n",
    "# )\n",
    "# print(f\" predicted coordinates is {landmarks_predicted_class3} and GT coordinate is {left_ijk_loaded_rw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fcfcf-af2f-43dd-94e0-58f1ef56d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_predicted_class1[0], left_ijk_loaded_apex#apex \n",
    "# landmarks_predicted_class2[0], left_ijk_loaded#basal\n",
    "# landmarks_predicted_class3[0], left_ijk_loaded_rw#round window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d458709c-5027-4e2b-83eb-0fea8888e015",
   "metadata": {},
   "source": [
    "### distance calculation: with 0.3mm image spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb9182-f056-461a-93c6-f26e110d97a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_apex = distance_mm(\n",
    "    v_loaded, landmarks_predicted_class1[0], cochlea_center_l_loaded_apex\n",
    ")\n",
    "distance_apex  # mm\n",
    "##can also calcuate the distance on other landmarks prediction\n",
    "# distance_basal = distance_mm(v_loaded, landmarks_predicted_class2[0], cochlea_center_l_loaded)\n",
    "# distance_basal\n",
    "# distance_rw = distance_mm(v_loaded, landmarks_predicted_class3[0], cochlea_center_l_loaded_rw)\n",
    "# distance_rw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbdbbd2-91db-4a28-a431-cced9e6645dd",
   "metadata": {},
   "source": [
    "### visualisation: compare with the ground truth landmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731dc868-9655-4903-a01a-5cb2f17b3088",
   "metadata": {},
   "source": [
    "Axial view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f971c-3a30-44c4-927b-25038c86cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## entire image \n",
    "slice_=np.int(left_ijk_loaded_apex[2])\n",
    "img = v_loaded[:,:,slice_]\n",
    "\n",
    "img = img.data.squeeze()\n",
    "plt.imshow(img, cmap='gray')\n",
    "predicted=landmarks_predicted_class1[0][:2]\n",
    "plt.scatter(predicted[1], predicted[0], color='g', label='Predicted',s=5)   \n",
    "gt=left_ijk_loaded_apex[:2]\n",
    "plt.scatter(gt[1], gt[0], color='r', label='Ground Truth',s=5)  \n",
    "legend = plt.legend(loc='upper right', fontsize='small', frameon=True, framealpha=1, facecolor='white', edgecolor='black')\n",
    "legend.get_frame().set_linewidth(0.5)\n",
    "plt.savefig('prediection_model11_apex_axial_entireimageview.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0856894-0577-4590-b64d-464874379b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## zoomed view\n",
    "img = v_loaded[:,:,slice_]\n",
    "img = img.data.squeeze()\n",
    "plt.imshow(img, cmap='gray')\n",
    "x = landmarks_predicted_class1[0][0]\n",
    "y= landmarks_predicted_class1[0][1]\n",
    "z= landmarks_predicted_class1[0][2]\n",
    "predicted = (x, y)\n",
    "plt.scatter(predicted[1], predicted[0], color='g', label='Predicted',s=10)   \n",
    "x_ = left_ijk_loaded_apex[0]\n",
    "y_= left_ijk_loaded_apex[1]\n",
    "z_= left_ijk_loaded_apex[2]\n",
    "gt=(x_, y_)\n",
    "plt.scatter(gt[1], gt[0], color='r', label='Ground Truth',s=10)  \n",
    "plt.xlim(predicted[1]-50, predicted[1]+50)# here to zoom if want to take closer look \n",
    "plt.ylim(predicted[0]-50, predicted[0]+50)\n",
    "legend = plt.legend(loc='upper right', fontsize='small', frameon=True, framealpha=1, facecolor='white', edgecolor='black')\n",
    "legend.get_frame().set_linewidth(0.5)\n",
    "plt.savefig('prediction_model11_apex_axial.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4417dbc-b5d6-451c-91ba-032065315cd0",
   "metadata": {},
   "source": [
    "Coronal view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028deaff-982d-4728-ac4e-dc9805254cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_=np.int(left_ijk_loaded_apex[1])\n",
    "img = v_loaded[:,slice_,:]\n",
    "img = img.data.squeeze()\n",
    "plt.imshow(img, cmap='gray')\n",
    "x = landmarks_predicted_class1[0][0]\n",
    "y= landmarks_predicted_class1[0][1]\n",
    "z= landmarks_predicted_class1[0][2]\n",
    "predicted = (x, z)\n",
    "plt.scatter(predicted[1], predicted[0], color='g', label='Predicted',s=10)   \n",
    "x_ = left_ijk_loaded_apex[0]\n",
    "y_= left_ijk_loaded_apex[1]\n",
    "z_= left_ijk_loaded_apex[2]\n",
    "gt=(x_, z_)\n",
    "plt.scatter(gt[1], gt[0], color='r', label='Ground Truth',s=10)  \n",
    "plt.xlim(predicted[1]-50, predicted[1]+50)# here to zoom if want to take closer look \n",
    "plt.ylim(predicted[0]-50, predicted[0]+50)\n",
    "legend = plt.legend(loc='upper right', fontsize='small', frameon=True, framealpha=1, facecolor='white', edgecolor='black')\n",
    "legend.get_frame().set_linewidth(0.5)\n",
    "plt.savefig('prediction_model11_apex_coronal.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d3947c-0fb1-42f8-9b5d-e97232ffad63",
   "metadata": {},
   "source": [
    "Sagittal view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02482465-21ca-4105-b170-c7d239448abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_=np.int(left_ijk_loaded_apex[0])\n",
    "img = v_loaded[slice_,:,:]\n",
    "img = img.data.squeeze()\n",
    "plt.imshow(img, cmap='gray')\n",
    "x = landmarks_predicted_class1[0][0]\n",
    "y= landmarks_predicted_class1[0][1]\n",
    "z= landmarks_predicted_class1[0][2]\n",
    "predicted = (y, z)\n",
    "plt.scatter(predicted[1], predicted[0], color='g', label='Predicted',s=10)   \n",
    "x_ = left_ijk_loaded_apex[0]\n",
    "y_= left_ijk_loaded_apex[1]\n",
    "z_= left_ijk_loaded_apex[2]\n",
    "gt=(y_, z_)\n",
    "plt.scatter(gt[1], gt[0], color='r', label='Ground Truth',s=10)  \n",
    "plt.xlim(predicted[1]-50, predicted[1]+50)# here to zoom if want to take closer look \n",
    "plt.ylim(predicted[0]-50, predicted[0]+50)\n",
    "legend = plt.legend(loc='upper right', fontsize='small', frameon=True, framealpha=1, facecolor='white', edgecolor='black')\n",
    "legend.get_frame().set_linewidth(0.5)\n",
    "plt.savefig('prediction_model11_apex_saggital.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_opensource",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
